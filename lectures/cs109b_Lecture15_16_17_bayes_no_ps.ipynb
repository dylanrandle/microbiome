{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyjags\n",
    "import numpy as np\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate from Be(2,5)\n",
    "theta = np.random.beta(2,5,10000) # generate 10,000 values from Be(2,5)\n",
    "print(theta[0:10]) # first ten simulated values\n",
    "print(np.mean(theta)) # sample mean\n",
    "print(np.percentile(theta,[2.5,97.5])) # 2.5% and 97.5% of empirical distn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate y from posterior predictive distribution\n",
    "theta = np.random.beta(2,5,10000) # generate 10,000 values from Be(2,5)\n",
    "print(theta[0:10]) # first ten simulated values\n",
    "y = np.random.binomial(1,theta,10000) # generate 10,000 binary values with different probabilities\n",
    "print(y[0:10]) #first ten values\n",
    "print(np.bincount(y)/10000)  # frequency of 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beetles_code = '''\n",
    "model {\n",
    "mean_x = mean(x);\n",
    "centered_x = x - mean_x;\n",
    "alpha = alpha_star - beta*mean_x; ## original alpha\n",
    "alpha_star ~ dnorm(0.0, 0.0001); ## prior for alpha_star\n",
    "beta ~ dnorm(0.0, 0.0001); ## prior for beta\n",
    "linpred = alpha_star + beta * centered_x;\n",
    "for (i in 1:N)  {\n",
    "    p[i] = ilogit(linpred[i]);\n",
    "    yhat[i] = p[i]*n[i];  ## fitted values\n",
    "    y[i] ~ dbin(p[i],n[i]) ## model for y\n",
    "  }\n",
    "}\n",
    "'''\n",
    "\n",
    "#An alternative would be to read the JAGS code from a separate file, as follows:\n",
    "#\n",
    "#with open(\"beetles.jag\",\"r\") as beetles_jags:\n",
    "#    beetles_code = beetles_jags.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beetles_x = np.array([1.6907, 1.7242, 1.7552, 1.7842, 1.8113, 1.8369, 1.8610, 1.8839])\n",
    "beetles_n = np.array([59, 60, 62, 56, 63, 59, 62, 60])\n",
    "beetles_y = np.array([6, 13, 18, 28, 52, 53, 61, 60])\n",
    "beetles_N = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beetles_model = pyjags.Model(beetles_code, data=dict(x=beetles_x, n=beetles_n, y = beetles_y, N=beetles_N), chains=4)\n",
    "beetles_burnin = beetles_model.sample(500, vars=['alpha_star','beta']) #warmup/burn-in\n",
    "beetles_samples = beetles_model.sample(2500, vars=['alpha_star','beta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpy2\n",
    "from rpy2.robjects.packages import importr\n",
    "#If there are errors about missing R packages, run the code below:\n",
    "#r_utils = importr(\"utils\")\n",
    "#r_utils.install_packages('coda')\n",
    "r_coda = importr(\"coda\")\n",
    "from rpy2.robjects import pandas2ri\n",
    "pandas2ri.activate()\n",
    "#chain 1\n",
    "beetles_chain1 = np.column_stack((beetles_samples['alpha_star'][0][:,0],beetles_samples['beta'][0][:,0]))\n",
    "beetles_chain1_df = pd.DataFrame({'alpha_star':beetles_chain1[:,0],'beta':beetles_chain1[:,1]})\n",
    "beetles_chain1_mcmc = r_coda.mcmc(pandas2ri.py2ri(beetles_chain1_df))\n",
    "#chain 2\n",
    "beetles_chain2 = np.column_stack((beetles_samples['alpha_star'][0][:,1],beetles_samples['beta'][0][:,1]))\n",
    "beetles_chain2_df = pd.DataFrame({'alpha_star':beetles_chain2[:,0],'beta':beetles_chain2[:,1]})\n",
    "beetles_chain2_mcmc = r_coda.mcmc(pandas2ri.py2ri(beetles_chain2_df))\n",
    "#chain 3\n",
    "beetles_chain3 = np.column_stack((beetles_samples['alpha_star'][0][:,2],beetles_samples['beta'][0][:,2]))\n",
    "beetles_chain3_df = pd.DataFrame({'alpha_star':beetles_chain3[:,0],'beta':beetles_chain3[:,1]})\n",
    "beetles_chain3_mcmc = r_coda.mcmc(pandas2ri.py2ri(beetles_chain3_df))\n",
    "#chain 4\n",
    "beetles_chain4 = np.column_stack((beetles_samples['alpha_star'][0][:,3],beetles_samples['beta'][0][:,3]))\n",
    "beetles_chain4_df = pd.DataFrame({'alpha_star':beetles_chain4[:,0],'beta':beetles_chain4[:,1]})\n",
    "beetles_chain4_mcmc = r_coda.mcmc(pandas2ri.py2ri(beetles_chain4_df))\n",
    "#convert to mcmc_list object\n",
    "beetles_chains=r_coda.mcmc_list(beetles_chain1_mcmc,beetles_chain2_mcmc,beetles_chain3_mcmc,beetles_chain4_mcmc)\n",
    "#get n_eff and Rhat\n",
    "beetles_n_eff = np.round(np.array(r_coda.effectiveSize(beetles_chains))) #round because must be an integer\n",
    "beetles_rhat = np.array(r_coda.gelman_diag(beetles_chains).rx2(\"psrf\"))\n",
    "beetles_rhat = np.array([beetles_rhat[0][0],beetles_rhat[1][0]]) #extract point estimates\n",
    "#calculate summary\n",
    "beetles_alpha_star_summary = [np.mean(beetles_samples['alpha_star']),np.std(beetles_samples['alpha_star'])]\n",
    "beetles_beta_summary = [np.mean(beetles_samples['beta']),np.std(beetles_samples['beta'])]\n",
    "for i in [0.025,0.25,0.5,0.75,0.975]:\n",
    "    beetles_alpha_star_summary.append(np.quantile(beetles_samples['alpha_star'],i))\n",
    "    beetles_beta_summary.append(np.quantile(beetles_samples['beta'],i))\n",
    "beetles_alpha_star_summary.append(beetles_n_eff[0])\n",
    "beetles_alpha_star_summary.append(beetles_rhat[0])\n",
    "beetles_beta_summary.append(beetles_n_eff[1])\n",
    "beetles_beta_summary.append(beetles_rhat[1])\n",
    "beetles_summary = pd.DataFrame([beetles_alpha_star_summary,beetles_beta_summary],columns=[\"mean\",\"sd\",\"2.5%\",\"25%\",\"50%\",\"75%\",\"97.5%\",\"n_eff\",\"Rhat\"],index=[\"alpha_star\",\"beta\"])\n",
    "beetles_summary.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(11,8.5))\n",
    "_ = plt.plot(range(500),beetles_burnin['alpha_star'][0,:,0],color=\"red\",label=\"Chain 1\")\n",
    "_ = plt.plot(range(500),beetles_burnin['alpha_star'][0,:,1],color=\"blue\",label=\"Chain 2\")\n",
    "_ = plt.plot(range(500),beetles_burnin['alpha_star'][0,:,2],color=\"cyan\",label=\"Chain 3\")\n",
    "_ = plt.plot(range(500),beetles_burnin['alpha_star'][0,:,3],color=\"green\",label=\"Chain 4\")\n",
    "_ = plt.xlabel(\"iteration\")\n",
    "_ = plt.ylabel(\"alpha_star\")\n",
    "_ = plt.title(\"Traceplot for Beetles data: alpha_star\")\n",
    "_ = plt.legend()\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(11,8.5))\n",
    "_ = plt.plot(range(501,3001),beetles_samples['alpha_star'][0,:,0],color=\"red\",label=\"Chain 1\")\n",
    "_ = plt.plot(range(501,3001),beetles_samples['alpha_star'][0,:,1],color=\"blue\",label=\"Chain 2\")\n",
    "_ = plt.plot(range(501,3001),beetles_samples['alpha_star'][0,:,2],color=\"cyan\",label=\"Chain 3\")\n",
    "_ = plt.plot(range(501,3001),beetles_samples['alpha_star'][0,:,3],color=\"green\",label=\"Chain 4\")\n",
    "_ = plt.xlabel(\"iteration\")\n",
    "_ = plt.ylabel(\"alpha_star\")\n",
    "_ = plt.title(\"Traceplot for Beetles data: alpha_star\")\n",
    "_ = plt.legend()\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(11,8.5))\n",
    "_ = plt.plot(range(500),beetles_burnin['beta'][0,:,0],color=\"red\",label=\"Chain 1\")\n",
    "_ = plt.plot(range(500),beetles_burnin['beta'][0,:,1],color=\"blue\",label=\"Chain 2\")\n",
    "_ = plt.plot(range(500),beetles_burnin['beta'][0,:,2],color=\"cyan\",label=\"Chain 3\")\n",
    "_ = plt.plot(range(500),beetles_burnin['beta'][0,:,3],color=\"green\",label=\"Chain 4\")\n",
    "_ = plt.xlabel(\"iteration\")\n",
    "_ = plt.ylabel(\"beta\")\n",
    "_ = plt.title(\"Traceplot for Beetles data: beta\")\n",
    "_ = plt.legend()\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(11,8.5))\n",
    "_ = plt.plot(range(501,3001),beetles_samples['beta'][0,:,0],color=\"red\",label=\"Chain 1\")\n",
    "_ = plt.plot(range(501,3001),beetles_samples['beta'][0,:,1],color=\"blue\",label=\"Chain 2\")\n",
    "_ = plt.plot(range(501,3001),beetles_samples['beta'][0,:,2],color=\"cyan\",label=\"Chain 3\")\n",
    "_ = plt.plot(range(501,3001),beetles_samples['beta'][0,:,3],color=\"green\",label=\"Chain 4\")\n",
    "_ = plt.xlabel(\"iteration\")\n",
    "_ = plt.ylabel(\"beta\")\n",
    "_ = plt.title(\"Traceplot for Beetles data: beta\")\n",
    "_ = plt.legend()\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sleepstudy = pd.read_csv(\"sleepstudy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "prev_subj = int(sleepstudy[['Subject']].iloc[0])\n",
    "sleepstudy_id = [1]\n",
    "for i in range(1,len(sleepstudy[['Subject']])):\n",
    "    if int(sleepstudy[['Subject']].iloc[i])==prev_subj:\n",
    "        sleepstudy_id.append(sleepstudy_id[-1])\n",
    "    else:\n",
    "        sleepstudy_id.append(sleepstudy_id[-1]+1)\n",
    "    prev_subj = int(sleepstudy[['Subject']].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleepstudy_code = '''\n",
    "model {\n",
    "mean_x = mean(x);\n",
    "for (i in 1:N){\n",
    "    linpred[i] = alpha[id[i]] + beta[id[i]]*x[i];\n",
    "}\n",
    "sig2_inv ~ dgamma(0.001, 0.001);\n",
    "sig = sqrt(1/sig2_inv);\n",
    "tau2_alpha_inv ~ dgamma(0.001, 0.0001);\n",
    "tau_alpha = sqrt(1/tau2_alpha_inv);\n",
    "tau2_beta_inv ~ dgamma(0.001, 0.001);\n",
    "tau_beta = sqrt(1/tau2_beta_inv);\n",
    "for (j in 1:N_ID){\n",
    "    alpha[j] ~ dnorm(300, tau2_alpha_inv);\n",
    "    beta[j] ~ dnorm(10, tau2_beta_inv);\n",
    "}\n",
    "for (i in 1:N){\n",
    "    y[i] ~ dnorm(linpred[i], sig2_inv);\n",
    "    }\n",
    "}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleepstudy_model = pyjags.Model(sleepstudy_code, data=dict(N = len(sleepstudy_id), N_ID = max(sleepstudy_id), y = flatten(np.array(sleepstudy[['Reaction']])), id = sleepstudy_id, x = flatten(np.array(sleepstudy[['Days']])) ), chains=3)\n",
    "_ = sleepstudy_model.sample(2000, vars = []) #warmup/burn-in\n",
    "sleepstudy_samples = sleepstudy_model.sample(5000, vars = [\"alpha\",\"beta\",\"linpred\",\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "#chain 1\n",
    "sleepstudy_chain1 = np.column_stack((np.column_stack([sleepstudy_samples['alpha'][i][:,0] for i in range(18)]),np.column_stack([sleepstudy_samples['beta'][i][:,0] for i in range(18)])))\n",
    "#the next line uses a dictionary comprehension and expression unpacking to merge dictionaries \n",
    "#see https://www.python.org/dev/peps/pep-0448/ as a reference for the latter\n",
    "sleepstudy_chain1_df = pd.DataFrame(OrderedDict({**{'alpha_'+str(i+1):sleepstudy_chain1[:,i] for i in range(18)},**{'beta_'+str(i-17):sleepstudy_chain1[:,i] for i in range(18,36)}}))\n",
    "sleepstudy_chain1_mcmc = r_coda.mcmc(pandas2ri.py2ri(sleepstudy_chain1_df))\n",
    "#chain 2\n",
    "sleepstudy_chain2 = np.column_stack((np.column_stack([sleepstudy_samples['alpha'][i][:,1] for i in range(18)]),np.column_stack([sleepstudy_samples['beta'][i][:,1] for i in range(18)])))\n",
    "sleepstudy_chain2_df = pd.DataFrame(OrderedDict({**{'alpha_'+str(i+1):sleepstudy_chain2[:,i] for i in range(18)},**{'beta_'+str(i-17):sleepstudy_chain2[:,i] for i in range(18,36)}}))\n",
    "sleepstudy_chain2_mcmc = r_coda.mcmc(pandas2ri.py2ri(sleepstudy_chain2_df))\n",
    "#chain 3\n",
    "sleepstudy_chain3 = np.column_stack((np.column_stack([sleepstudy_samples['alpha'][i][:,2] for i in range(18)]),np.column_stack([sleepstudy_samples['beta'][i][:,2] for i in range(18)])))\n",
    "sleepstudy_chain3_df = pd.DataFrame(OrderedDict({**{'alpha_'+str(i+1):sleepstudy_chain3[:,i] for i in range(18)},**{'beta_'+str(i-17):sleepstudy_chain3[:,i] for i in range(18,36)}}))\n",
    "sleepstudy_chain3_mcmc = r_coda.mcmc(pandas2ri.py2ri(sleepstudy_chain3_df))\n",
    "#convert to mcmc_list object\n",
    "sleepstudy_chains=r_coda.mcmc_list(sleepstudy_chain1_mcmc,sleepstudy_chain2_mcmc,sleepstudy_chain3_mcmc)\n",
    "#get n_eff and Rhat\n",
    "sleepstudy_n_eff = np.round(np.array(r_coda.effectiveSize(sleepstudy_chains))) #round because must be an integer\n",
    "sleepstudy_rhat = np.array(r_coda.gelman_diag(sleepstudy_chains).rx2(\"psrf\"))\n",
    "sleepstudy_rhat = np.array([sleepstudy_rhat[i][0] for i in range(36)]) #extract point estimates\n",
    "#calculate summary\n",
    "sleepstudy_alpha_summary = [[np.mean(sleepstudy_samples['alpha'][i]),np.std(sleepstudy_samples['alpha'][i])] for i in range(18)]\n",
    "sleepstudy_beta_summary = [[np.mean(sleepstudy_samples['beta'][i]),np.std(sleepstudy_samples['beta'][i])] for i in range(18)]\n",
    "for i in [0.025,0.25,0.5,0.75,0.975]:\n",
    "    for j in range(18):\n",
    "        sleepstudy_alpha_summary[j].append(np.quantile(sleepstudy_samples['alpha'][j],i))\n",
    "        sleepstudy_beta_summary[j].append(np.quantile(sleepstudy_samples['beta'][j],i))\n",
    "for j in range(18):\n",
    "    sleepstudy_alpha_summary[j].append(sleepstudy_n_eff[j])\n",
    "    sleepstudy_alpha_summary[j].append(sleepstudy_rhat[j])\n",
    "    sleepstudy_beta_summary[j].append(sleepstudy_n_eff[j+18])\n",
    "    sleepstudy_beta_summary[j].append(sleepstudy_rhat[j+18])\n",
    "sleepstudy_summary = pd.DataFrame(sleepstudy_alpha_summary+sleepstudy_beta_summary,columns=[\"mean\",\"sd\",\"2.5%\",\"25%\",\"50%\",\"75%\",\"97.5%\",\"n_eff\",\"Rhat\"],index=['alpha_'+str(i+1) for i in range(18)]+['beta_'+str(i+1) for i in range(18)])\n",
    "sleepstudy_summary.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ysleep = np.array(flatten(np.array(sleepstudy[['Reaction']])))\n",
    "ysleep_pred = np.mean(np.mean(sleepstudy_samples['linpred'],axis=1),axis=1)\n",
    "_ = plt.figure(figsize=(11,8.5))\n",
    "_ = plt.scatter(ysleep,ysleep_pred)\n",
    "_ = plt.title(\"Observed and fitted reaction times from Bayesian hierarchical linear model\")\n",
    "_ = plt.xlabel(\"Observed reaction time (ms)\")\n",
    "_ = plt.ylabel(\"Predicted reaction time (ms)\")\n",
    "minval = min(np.min(ysleep),np.min(ysleep_pred))\n",
    "maxval = max(np.max(ysleep),np.max(ysleep_pred))\n",
    "_ = plt.plot([minval,maxval],[minval,maxval],color=\"black\")\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "import seaborn as sns\n",
    "from matplotlib import gridspec\n",
    "i = 0\n",
    "plt.figure(figsize=(11,8.5))\n",
    "gs  = gridspec.GridSpec(3, 6)\n",
    "gs.update(wspace=0.5, hspace=0.5)\n",
    "for subj in np.unique(sleepstudy['Subject']):\n",
    "    ss_extract = sleepstudy.loc[sleepstudy['Subject']==subj]\n",
    "    ss_extract_ols = sm.ols(formula=\"Reaction~Days\",data=ss_extract).fit() #if we want to analyze fit later\n",
    "    #new subplot\n",
    "    plt.subplot(gs[i])\n",
    "    #plot without confidence intervals\n",
    "    sns.regplot(x='Days', y='Reaction', ci=None, data=ss_extract).set_title('Subject '+str(subj))\n",
    "    if i not in [0,6,12]:\n",
    "        plt.ylabel(\"\")\n",
    "    i+=1\n",
    "_ = plt.figlegend(['per-subject'],loc = 'lower center', ncol=6)\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "plt.figure(figsize=(11,8.5))\n",
    "for subj in np.unique(sleepstudy['Subject']):\n",
    "    ss_extract = sleepstudy.loc[sleepstudy['Subject']==subj]\n",
    "    #new subplot\n",
    "    plt.subplot(gs[i])\n",
    "    #plot without confidence intervals\n",
    "    sns.regplot(x='Days', y='Reaction', ci=None, data=ss_extract).set_title('Subject '+str(subj)) \n",
    "    sns.regplot(x='Days', y='Reaction', ci=None, scatter=False, data=sleepstudy)\n",
    "    if i not in [0,6,12]:\n",
    "        plt.ylabel(\"\")\n",
    "    i+=1\n",
    "_ = plt.figlegend(['per-subject','pooled'],loc = 'lower center', ncol=6)\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "plt.figure(figsize=(11,8.5))\n",
    "subj_arr = np.unique(sleepstudy['Subject'])\n",
    "for subj in subj_arr:\n",
    "    ss_extract = sleepstudy.loc[sleepstudy['Subject']==subj]\n",
    "    #new subplot\n",
    "    plt.subplot(gs[i])\n",
    "    #plot without confidence intervals\n",
    "    sns.regplot(x='Days', y='Reaction', ci=None, data=ss_extract).set_title('Subject '+str(subj)) \n",
    "    sns.regplot(x='Days', y='Reaction', ci=None, scatter=False, data=sleepstudy)\n",
    "    subj_num = int(np.where(subj_arr==subj)[0])\n",
    "    hmodel_fit = [np.mean(sleepstudy_samples['alpha'][subj_num])+np.mean(sleepstudy_samples['beta'][subj_num])*x for x in range(-1,11)]\n",
    "    sns.lineplot(x=range(-1,11),y=hmodel_fit)\n",
    "    if i not in [0,6,12]:\n",
    "        plt.ylabel(\"\")\n",
    "    i+=1\n",
    "_ = plt.figlegend(['per-subject','pooled','hierarchical'],loc = 'lower center', ncol=6)\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = {}\n",
    "def readbook(booknum):\n",
    "    with open(\"harrypotter_book\"+str(booknum)+\".txt\", \"r\") as book: \n",
    "        books[booknum] = book.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,8):\n",
    "    readbook(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "for i in books:\n",
    "    books[i] = re.split(\"Chapter\\s?\\d*|Epilogue\",books[i]) #split each book into chapters\n",
    "    #note that coming up with this regular expression required some trial and error (e.g. finding the epilogue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#references on LDA in Python: \n",
    "#https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html\n",
    "#https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['harry','hermione','ron']) #add stop words for three main characters\n",
    "stop_words.update(['said','got','get','would','could']) #empirically find these words very common\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'[\\w\\']+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for book in books:\n",
    "    for i in range(len(books[book])):\n",
    "        #make everything lower case\n",
    "        books[book][i] = books[book][i].lower()\n",
    "        #tokenize\n",
    "        books[book][i] = tokenizer.tokenize(books[book][i])\n",
    "        #remove stop words\n",
    "        books[book][i] = [j for j in books[book][i] if j not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate chapters over all books\n",
    "masterbooks = []\n",
    "for book in books:\n",
    "    for chapter in books[book]:\n",
    "        masterbooks.append(chapter)\n",
    "masterdictionary = {}\n",
    "mastercorpus = {}\n",
    "masterdictionary = Dictionary(masterbooks)\n",
    "mastercorpus = [masterdictionary.doc2bow(chapter) for chapter in masterbooks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvec = CountVectorizer()\n",
    "X = countvec.fit_transform(flatten(masterbooks))\n",
    "Xsum = X.sum(axis=0)\n",
    "words_freq = [(word, Xsum[0, idx]) for word, idx in countvec.vocabulary_.items()]\n",
    "words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "words_freq[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding optimal number of topics via coherence measure\n",
    "coherence_vals = []\n",
    "for ntop in range(1,31):\n",
    "   mod = LdaModel(mastercorpus, num_topics = ntop, id2word = masterdictionary, passes=10)\n",
    "   cmod = CoherenceModel(model=mod, corpus=mastercorpus, dictionary=masterdictionary, coherence='u_mass')\n",
    "   cval = cmod.get_coherence()\n",
    "   print(ntop,cval)\n",
    "   coherence_vals.append(cval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(11,8.5))\n",
    "_ = plt.plot(range(1,31),coherence_vals)\n",
    "_ = plt.xlabel(\"Number of Topics\")\n",
    "_ = plt.ylabel(\"Coherence Score\")\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterldamodel = LdaModel(mastercorpus, num_topics=16, id2word = masterdictionary, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = [[word for word,_ in masterldamodel.show_topic(topicno, topn=50)] for topicno in range(masterldamodel.num_topics)]\n",
    "top_betas = [[beta for _,beta in masterldamodel.show_topic(topicno, topn=50)] for topicno in range(masterldamodel.num_topics)]\n",
    "print(\"Top Topics:\")\n",
    "for topicno, words in enumerate(top_words):\n",
    "    print(\"%i: %s\" % (topicno, ' '.join(words[:15])))\n",
    "print(\"Top Topic Betas:\")\n",
    "for topicno, betas in enumerate(top_betas):\n",
    "    print(\"%i: %s\" % (topicno, ' '.join(map(str,betas[:15]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words[0][:5]\n",
    "top_betas[0][:5]\n",
    "gs  = gridspec.GridSpec(2,3)\n",
    "gs.update(wspace=0.5, hspace=0.5)\n",
    "plt.figure(figsize=(11,8.5))\n",
    "for i in range(6):\n",
    "    #new subplot\n",
    "    ax = plt.subplot(gs[i])\n",
    "    plt.barh(range(5), top_betas[i][:5], align='center',color='blue', ecolor='black')\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_yticks(range(5))\n",
    "    ax.set_yticklabels(top_words[i][:5])\n",
    "    plt.title(\"Topic \"+str(i))\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = masterldamodel.inference(mastercorpus)[0]\n",
    "#note that dimensions of theta are number of documents (here, chapters) x number of topics in LDA model\n",
    "for row in range(len(theta)): #normalize rows\n",
    "    theta[row] = theta[row]/sum(theta[row])\n",
    "theta_df = pd.DataFrame(theta)\n",
    "#first column is document number (chapter over all Harry Potter books), second column is topic number\n",
    "theta_df.stack().nlargest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pyldavis (if not already installed)\n",
    "# note that the dynamic visualizations typically require an internet connection in order to work\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim \n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mastervis = pyLDAvis.gensim.prepare(masterldamodel, mastercorpus, masterdictionary)\n",
    "mastervis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#appendix:\n",
    "#separate LDA models for each book\n",
    "dictionary = {}\n",
    "corpus = {}\n",
    "for book in books:\n",
    "    dictionary[book] = Dictionary(books[book])\n",
    "    corpus[book] = [dictionary[book].doc2bow(chapter) for chapter in books[book]]\n",
    "ldamodel = {}\n",
    "for i in books:\n",
    "    ldamodel[i] = LdaModel(corpus[i], num_topics=16, id2word = dictionary[i], passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visbook1 = pyLDAvis.gensim.prepare(ldamodel[1], corpus[1], dictionary[1])\n",
    "visbook1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
