{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Hyperparameter Optimization\n",
    "\n",
    "Due to the sensitivity of the cLSTM's hyperparameters, we use Bayesian optimization to discover a high-performing set of training and regularization hyperparameter values. Bayesian optimization is a natural framework for hyperparameter search. It excels at optimization under uncertainty, particularly with noisy black-box functions that are expensive to evaluate, such as training and evaluating neural networks.\n",
    "\n",
    "Under the Bayesian optimization framework, we wish to identify a set of hyperparameters $\\theta^\\ast$ such that,\n",
    "\n",
    "$$\n",
    "\\theta^\\ast \\approx \\arg\\max_\\theta \\sigma(f(\\mathcal{D};\\theta))\n",
    "$$\n",
    "\n",
    "where $f$ is the model whose hyperparameters we wish to tune and $\\sigma(\\cdot)$ is a scoring model on $f$. In other words, we wish to find the hyperparameters $\\theta$ of a model that maximizes its score (defined below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll load in the data and prepare it for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HUBxX-xWi41I"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.stats import beta\n",
    "from utils import load_data\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt.observer import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "\n",
    "from clstm import cLSTM, train_model_gista, train_model_adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kjRRddcRjEyl"
   },
   "outputs": [],
   "source": [
    "mice = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "19m50JJpjFsx"
   },
   "outputs": [],
   "source": [
    "mouse2 = mice[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yb6G9hx7jGlj"
   },
   "outputs": [],
   "source": [
    "# percent reads / times\n",
    "mouse2_pct = mouse2['reads_percent']\n",
    "mouse2_abs = mouse2['reads_abs_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UrKfq31IjTZv"
   },
   "outputs": [],
   "source": [
    "IMP_READS = np.arange(20)\n",
    "top_reads_pct = mouse2_pct[IMP_READS, :].T\n",
    "top_reads_abs = mouse2_abs[IMP_READS, :].T\n",
    "\n",
    "mean_abs = np.mean(top_reads_abs, axis=0)\n",
    "std_abs = np.std(top_reads_abs, axis=0)\n",
    "top_reads_abs = (top_reads_abs - mean_abs) / std_abs\n",
    "\n",
    "X_torch_pct = torch.tensor(top_reads_pct[np.newaxis], dtype=torch.float32)\n",
    "X_torch_abs = torch.tensor(top_reads_abs[np.newaxis], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define the score model $\\sigma$. The score function is tricky in this context because we do not actually have a concrete way of quantifying how well a model is doing. Since we're interested in the Granger Causality coefficients and not the overall predictive power, just using the MSE loss would not give us the results we want. However, we can encode the belief that we expect there to be some causal links between bacteria. We also believe that causal links should generally be more prevalant along the diagonal. This led us to develop the following score function:\n",
    "\n",
    "$$\n",
    "\\sigma(C) = \\text{Beta}(\\mathbb{E}[C]; \\alpha, \\beta) + \\mathbb{E}[\\text{diag}\\{C\\}] \\cdot \\left(1 - \\prod_i \\mathbb{1}_{c_i = 1}\\right)\n",
    "$$\n",
    "\n",
    "where we use $\\alpha = \\beta = 1.6$, and $C \\in \\{0, 1\\}^{NxN}$ is the collection of Granger Causality terms. In other words, we reward models with a mixture of zero and non-zero GC terms, and we further reward models with diagonal GC terms.\n",
    "\n",
    "This function is imprecise, but allows us to run optimization in order to find models that provide interesting results. In practice, non-lienar models are extremely difficult to train and most get all non-zero or all zero GC values. Thus, any model that gets a non-zero score with the above function is potentially of value to us. Whether or not it is precisely calibrated, it is useful in helping us discover hyperparameter confugirations that are useful to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gc_score(gc):\n",
    "    score = beta(a=1.6, b=1.6).pdf(gc.mean())\n",
    "    score += gc.diagonal().mean() * (gc.mean() != 1.)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ixWqfq6NK1lZ"
   },
   "outputs": [],
   "source": [
    "heatmaps = []\n",
    "def evaluate(n_hidden, lr_scale, lam_scale, lam_ridge_scale, truncation, data=X_torch_abs, max_iter=2500):\n",
    "    \"\"\" \n",
    "    Evaluate a given set of hyperparameters, training a model for 2500 epochs\n",
    "    and returning a score.\n",
    "    \n",
    "    This is the black-box function which our Bayesian Optimization algorithm\n",
    "    will attempt to optimize\n",
    "    \"\"\"\n",
    "    # transform continues values into valid hyperparams\n",
    "    n_hidden = int(n_hidden + .5)\n",
    "    truncation = int(truncation + .5)\n",
    "    lr = 10**lr_scale\n",
    "    lam = 10**lam_scale\n",
    "    lam_ridge = 10**lam_ridge_scale\n",
    "    \n",
    "    # train the model\n",
    "    gcmodel = cLSTM(p, n_hidden)\n",
    "    gcmodel.to('cuda')\n",
    "    train_loss_list, train_mse_list = train_model_gista(\n",
    "        gcmodel,\n",
    "        data.to('cuda'),\n",
    "        lam=lam,\n",
    "        lam_ridge=lam_ridge,\n",
    "        lr=lr,\n",
    "        max_iter=max_iter,\n",
    "        check_every=100,\n",
    "        truncation=truncation,\n",
    "        verbose=1\n",
    "    )\n",
    "    gc = gcmodel.GC(threshold=False).cpu().detach().numpy()\n",
    "    heatmaps.append(gc)\n",
    "    eval_heatmaps.append(gc)\n",
    "    gc_thresh = (heatmaps[-1] > 0).astype('float')\n",
    "    \n",
    "    # return the resulting model's score\n",
    "    return get_gc_score(gc_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define the range of hyperparameters which the algorithm should search over. We optimize the following ranges:\n",
    "\n",
    "- **\\# of hidden LSTM nodes:** min: 10, max: 256\n",
    "- **Learning rate:** min: .00001, max: .01 (searched on log 10 scale)\n",
    "- **GC sparsity penalty:**, min: .000001, max: 1 (searched on log 10 scale)\n",
    "- **LSTM Ridge term:** min: .000001, max: 1 (searched on log 10 scale)\n",
    "- **Time series window:** min: 3, max: 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TMO4X_Hv4I5v"
   },
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    'n_hidden': (10, 256),\n",
    "    'lr_scale': (-2, -5),\n",
    "    'lam_scale': (0, -6),\n",
    "    'lam_ridge_scale': (0, -6),\n",
    "    'truncation': (3, 20)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PN9-Yh8P4wXa"
   },
   "outputs": [],
   "source": [
    "optimizer = BayesianOptimization(\n",
    "    f=evaluate,\n",
    "    pbounds=pbounds,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then optimize using the Expected Improvement acquisition function, which essentially chooses new hyperparameters based on which point in hyperparameter space has the highest expected improvement over the maximum score we have seen so far.\n",
    "\n",
    "Since each point is extremely expensive to evaluate, we plot 5 initial random values and then 12 points selected according to our acquistion, saving the results in a `.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6gbp5SEr47nS"
   },
   "outputs": [],
   "source": [
    "logger = JSONLogger(path=\"./logs.json\")\n",
    "optimizer.subscribe(Events.OPTMIZATION_STEP, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kZKTj8HO49bn"
   },
   "outputs": [],
   "source": [
    "optimizer.maximize(\n",
    "    init_points=5,\n",
    "    n_iter=12,\n",
    "    acq='ei'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': 1.9629146090709826,\n",
       " 'params': {'lam_ridge_scale': -5.420411491727301,\n",
       "  'lam_scale': -0.8248482248774252,\n",
       "  'lr_scale': -2.4178290417522756,\n",
       "  'n_hidden': 208.61825702254285,\n",
       "  'truncation': 8.555859902725139}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model we found had a score of `1.96` (which is largely uninterpretable). It seemed to learn very small LSTM regression terms ($10^{-5.42}=0.0000038$) and relatively large terms for the GC sparsity regularization ($10^{-.82}=0.15$), as well as a window size of $9$ and $209$ hidden LSTM units, trained with a learning rate of $10^{-2.41}=Â 0.003$. In our experiments, we found this hyperparameter configuration did indeed give us well-performing models whose GC values were relatively consistent across experiments."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Neural_GC_Round2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
